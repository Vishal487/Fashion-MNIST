# -*- coding: utf-8 -*-
"""Fashion_mnist_iitpkd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H3Xd1wfSGklLViHQb2SvNaZuiCH3r_Xv
"""

# unzip the dataset zip file (commenting out bcz we don't need to do it agin and again)

# !unzip "drive/My Drive/fashion-mnist_train.zip" -d "drive/My Drive/"

# path to train csv file:-
# drive/My Drive/fashion-mnist_train.csv

# required library

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense
from keras.optimizers import RMSprop, SGD, adam

from sklearn.model_selection import train_test_split

"""### Data Preprocessing"""

# load the training dataset

train_csv = pd.read_csv('drive/My Drive/fashion-mnist_train.csv')
train_csv.head()

train_csv.shape

# checking for any NAN/missing value(s)

sum(train_csv.isna().sum())     # no missing value

# split into X(feature) and y(target)

X = train_csv.drop('label', axis=1)
y = train_csv['label']

X.shape, y.shape

# check for how many sample belongs to each class
y.value_counts()          # wow! super balanced

# normalization:   0-255 => 0-1
X = X / 255.0

X.values

# now reshape the images     784 => (28, 28, 1)          bcz of grayscale...
X = X.values.reshape(-1, 28, 28, 1)

X.shape

# label encoding

y = keras.utils.np_utils.to_categorical(y, num_classes=10)

y.shape

"""**We're ready with X_train and y_train**

### Create and build the Model
"""

# let's split into training and validation set
np.random.seed(42)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)

X_train.shape, y_train.shape, X_val.shape, y_val.shape

# Build the model

model = Sequential()
model.add(Conv2D(32, (5,5),
                 padding='same',
                 activation='relu',
                 input_shape=(28, 28, 1)))
model.add(Conv2D(32, (5,5),
                 padding='same',
                 activation='relu'))
model.add(MaxPool2D((2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3,3),
                 padding='same',
                 activation='relu'))
model.add(Conv2D(64, (3,3),
                 padding='same',
                 activation='relu'))
model.add(MaxPool2D((2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(10, activation='softmax'))

model.summary()

# compile the model

model.compile(optimizer=RMSprop(),
               loss='categorical_crossentropy',
               metrics=['accuracy'])

# training the model

batch_size = 86
epochs = 30

history = model.fit(X_train, y_train,
                    batch_size = batch_size,
                    epochs = epochs,
                    validation_data = (X_val, y_val),
                    verbose=2)

history.history.keys()

# Let's plot the loss and accuracy

plt.plot(np.arange(epochs), history.history['loss'], color='orange', label='Training loss')
plt.plot(np.arange(epochs), history.history['val_loss'], color='blue', label='Validation loss')
plt.title('Loss')
plt.xlabel('#Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.plot(np.arange(epochs), history.history['accuracy'], color='orange', label='Training accuracy')
plt.plot(np.arange(epochs), history.history['val_accuracy'], color='blue', label='Validation accuracy')
plt.title('Accuracy')
plt.xlabel('#Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""**Okay now training our model on full training dataset**

after enough hyperparameter tuning we got above thing and now we'll train on full dataset
"""

# note that (X,y) is full training dataset
# and now we don't have validation dataset anymore

# Rebuilding the same model
model = Sequential()
model.add(Conv2D(32, (5,5),
                 padding='same',
                 activation='relu',
                 input_shape=(28, 28, 1)))
model.add(Conv2D(32, (5,5),
                 padding='same',
                 activation='relu'))
model.add(MaxPool2D((2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3,3),
                 padding='same',
                 activation='relu'))
model.add(Conv2D(64, (3,3),
                 padding='same',
                 activation='relu'))
model.add(MaxPool2D((2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(10, activation='softmax'))
model.summary()




# compile the model
model.compile(optimizer=RMSprop(),
               loss='categorical_crossentropy',
               metrics=['accuracy'])



# fit the model ON TOTAL TRAINING SET AND NO VALIDATION SET
history = model.fit(X, y,
                    batch_size = batch_size,
                    epochs = epochs,
                    verbose=2)

"""### Preparing for submission"""

# load the test dataset

test = pd.read_csv('drive/My Drive/Q2_Clothing_test.csv')
test.head()

# making the same format as training set

# normalize
test = test / 255.0

# reshape
test = test.values.reshape(-1, 28, 28, 1)

test.shape

# making prediction on test dataset

prediction = model.predict(test)

# let's see prediction made on first image
prediction[0]

# let's visualize first 10 prediction

class_name = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',
              'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

for i in range(10):
    im = np.array(test[i]).reshape(28, 28)
    plt.imshow(im)
    plt.title(class_name[np.argmax(prediction[i])])
    plt.show()

# getting exact prediction

submission = []
for pred in prediction:
    submission.append(np.argmax(pred))

# making submission dataframe

submission_fashion_mnist = pd.DataFrame()
submission_fashion_mnist['class'] = submission 
submission_fashion_mnist.head()

# now save pandas DF as csv file
submission_fashion_mnist.to_csv('drive/My Drive/submission_fashion_mnist.csv', index=False)

THANK YOU!!!

